---
layout: post
title: An introduction to Pandas  
description: A short introduction to Pandas
date: 2019-10-20
author: Saeid Amiri
published: true
tags: pandas apply assign columns concat crosstab DataFrame  dtypes describe drop  fillna groupby head info iloc interpolate loc merge  ndim notnull null Pipeline read_csv rename to_csv to_excel replace reset.index save set_index shape sort_values tail where
categories: Pandas
comments: false
---

## Contents
It is a self-study to teach how use Pandas to work with database.
- [pandas](#pandas)
  - [data-frame](#data-frame)
  - [Adding new column](adding-new-column)
  - [Running function on row or column](#Running-function-on-row-or-column)
  - [Loading CSV data](#loading-csv-data)
  - [Generating summary](#generating-summary)
  - [Size of data-frame](#size-of-data-frame)
  - [Preview](#preview)
  - [Manipulating data-frame](#manipulating-data-frame)
  - [Merging](#merging)
  - [Crosstab](#crosstab)
  - [Missing Value](#missing-Value)
  - [Pipeline](#pipeline)
  - [Save](#save)
  - [Good References](#good-references)

# Pandas
Panad is built on top of Numpy and its functions are very useful for working with data set. The shortstand for uploading this library is <br>
```import pandas as pd```.

## Data-frame
data-frame via pandas is very useful format for working with data set, its structure is two-dimensional size-mutable, potentially heterogeneous
tabular data structure with labeled axes (rows and columns). The following codes create a data-frame from a dictionary.  

```
var={"A": [1,2,0], "B": [2,3,4]}
df= pd.DataFrame(data=var,index=['A', 'Z', 'C'])
```

The label column can be easily changed:
```
raw_data = {'population': [ 1015.0, 1129.0, 333.0,  515.0],'median_income': [ 1.5, 1.8,  1.7,  3.2]}
df=pd.DataFrame(raw_data, columns = ['population', 'median_income'])
```

In some circumstances, it is better to consider the time of collecting data as index, the following script changes the data to the time format, then save them as index.  
```
df = df.set_index(pd.to_datetime(['2019-04-01','2019-05-04','2019-06-01','2019-07-02']))
```

To create an empty data-frame, run the following
```
df1=pd.DataFrame(columns = ['population', 'median_income'])
df2=pd.DataFrame()
```

## Adding new column
A column can easily adding to data-frame 
```
df0=pd.DataFrame([38,40,25,33])
df['Ave_hour']=df0
```

Using ```assign() ``` can also add new columns, new columns can be generated using functions, see below
```
df=df.assign(Ave_hour=df0)
df=df.assign(PI1=lambda x: x['population']*x['median_income'],PI2=df['population']/df['median_income'] )
```

A new columns can be added to data-frame
```
df.columns=['population1','median_income','Ave_hour','PI1','PI2']
df=df.rename(columns={'population1': 'pop', 'median_income': 'med_income'})
```

## Running function on row or column
Using ```df.apply(fun)``` can apply a function on columns or row:
```
df.apply(np.sum, axis=0)
df.apply(np.sum, axis=1)
```

Even can write run a function on columns or rows
```
def prod(col):
    return col['A'] * col['B']

df.apply(prod, axis=1)
df['productcolmn']=df.apply(prod, axis=1)
```

## Loading CSV data
The function ```pd.read_csv``` imports the data saved in CSV format, the following show how imports the CSV file to Python, the data is California housing,  
```
source ="https://storage.googleapis.com/mledu-datasets/california_housing_train.csv"
CHT = pd.read_csv(source, sep=",")
CHT.head()
```

## Generating summary
To see the type, the information and summary of variables in the data-frame, use ```.dtypes```,  ```.describe()```, and   ```.info()```

```
# show the type of variables
CHT.dtypes
# generate summary
CHT.describe()
CHT.info()
```

It is easy to find the duplicates in data-frame  and  drop them 
```
CHT.duplicated()
CHT.drop_duplicates()
```
To check the duplication in variables, specify their names as well, 
```
CHT.duplicated(['longitude'])
CHT.drop_duplicates(['longitude'], keep='last')
CHT.index.duplicated()
```


## Size of data-frame
Dimension of data-frame is 2 which can be seen via ```.ndim ```, the number of rows and columns can be obtained by   ```.shape```.  

```
CHT.ndim
CHT.shape
CHT.shape[0]
CHT.shape[1]
```

## Preview
Beside the function print, pandas can show the first and the last part of data, using ```.head()``` and `.tail()`. By passing a number in the parenthesis, one can specify the output.

```
CHT.head(10)
CHT.tail(10)
CHT.sort_values(by='housing_median_age', ascending=False).head(3)
CHT.columns
```

## Manipulating data-frame
To select the data use the name of variable, or specify the indices via `.iloc` and `.loc` (link)[http://pandas.pydata.org/pandas-docs/version/0.22/indexing.html].  `.iloc` is an integer-based and select should use integer index. On contrary, `.loc`   is primarily label based, and may also be used with a boolean array.


```
CHT.longitude
CHT['longitude']
CHT.iloc[:,1]
CHT.iloc[:,[1,3]]
```

To select part of row, you can also use iloc[idenx of row,:], also rows can be selected using the logical values

```
CHT.iloc[2:10]
CHT.iloc[2:10,:]
CHT[CHT.iloc[:,1]<34]
```

To retieve part of row, should pass boolean variable, ```.iloc``` does not work boolean variable, and ```.loc``` should be used.  Consider the median_income in our data, by using quartile divid it into three categories.

```
CHT['famlev'] = ''
C1=CHT.median_income<=CHT.median_income.quantile(.3)
C2=CHT.median_income>=CHT.median_income.quantile(.7)
CHT.loc[C1,'famlev']='L'
CHT.loc[~C1&~C2,'famlev']='M'
CHT.loc[C2,'famlev']='H'
```
In this case we used `.loc`, obviously we specify column labels to retrieve columns instead of by position. Note: You can also using [][] apply different conditions on data.  

```
CHT['median_house_value'][CHT['famlev'] == 'M'].mean()
```

Selecting or searching can be done also using ```np.where``` which evaluate the condition 
```
CHT_R=CHT[['total_rooms','total_bedrooms']]
CHT_R.where(CHT.total_rooms<1000)
CHT_R.where(CHT.total_rooms<1000,0)
con= CHT_R<1000
CHT_R.where(con, -999)
```
Opposite of this `np.where` is `np.mask`, replace it with `np.where` and rerun the codes. 

To drop row and columns use ```.drop```. The ```np.where``` can be used to create a new  column, 
```
CHT['size']=np.where(CHT.total_rooms<1000, 'small', 'big')
CHT_R=CHT[['total_rooms','total_bedrooms']]
CHT_R.where(CHT.total_rooms<1000)
CHT_R.where(CHT.total_rooms<1000,0)
con= CHT_R<1000
CHT_R.where(con, -999)
```
Find which ones are 'M'
np.where(CHT.loc[:,'famlev'].isin(['M']))

```
CHT.drop([0,5], axis=0)
CHT.drop('longitude',axis=1, inplace=True)
```

To replace values, use `df.replace()`

```
CHT['famlev'].replace('L','Low').replace('M','Middle').replace('H','High')
CHT.drop('longitude',axis=1, inplace=True)
```

Note: the argument ```inplace=True``` apply the change on the original data.  


### list comprehension
Simple operation using the list comprehension can be done on data-frame as well.

CHT['NN']=[0 for x in CHT['total_rooms'] if x<100]
CHT['size']=['small' if x<100  else 'big'  for x in CHT['total_rooms']]

### Summarising
Although `.describe` can give a summary of variables,  more specific summery of variables (columns) can be extracted, see

```
CHT.count
CHT[CHT.iloc[:,1]<34].nunique()
```

The following table includes the functions.

|Function|Description|
|---|---|
`count`|	Number of non-null observations
`sum`	 | Sum of values
`mean`	|Mean of value
`mad` |	Mean absolute deviation
`median`|	median of values
`min`	 |Minimum
`max`	 |Maximum
`mode`	|Mode
`abs`	| Absolute Value
`prod`	| Product of values
`std`	 |Unbiased standard deviation
`var`	 |Unbiased variance
`sem`	 |Unbiased standard error of the mean
`skew`	| Unbiased skewness (3rd moment)
`kurt`	| Unbiased kurtosis (4th moment)
`quantile`	| Sample quantile (value at %)
`cumsum`	| Cumulative sum
`cumprod`| 	Cumulative product
`cummax`	| Cumulative maximum
`cummin`	| Cumulative minimum
`nunique`| number of unique elements
`value_counts`| Counts of unique values
`cov`| Calculate the covariance between columns
`corr`| Calculate the correlation between columns

The summaries can be obtained using any grouping variables in the data set:

```
CHT.groupby(['famlev']).groups.keys()
CHT.groupby(['famlev']).groups['H']
CHT.groupby(['famlev']).first()

CHT.groupby(['famlev']).sum()

CHT.groupby(['famlev'])['median_house_value'].sum()
# better output
CHT.groupby(['famlev'])[['median_house_value']].sum()
```

The grouped variables would be assigned as indices, to bring them back as variables use `pf.reset.index()`
```
CHT.reset_index()
```

It is possible to apply even complex function, the following scripts calculate the coefficient of data.

```
def cv(x):
 return (np.mean(x)/np.var(x))

aggr = {
    'total_rooms':'sum',
    'population': lambda x: cv(x)
}
CHT.groupby('famlev').agg(aggr)
```

The output can be tidied up,

```
aggr = {
    'total_rooms':['mean','std']
}
grouped = CHT.groupby('famlev').agg(aggr)
grouped.columns = grouped.columns.droplevel(level=0)
grouped.rename(columns={"mean": "total_rooms", "std": "total_rooms"})
grouped.head()
```

The summarizations can be done using pivot table, 
```
pd.pivot_table(CHT, index=['famlev'], aggfunc=['mean'])
```


## Merging
Panada is very useful for merging dataset, to achieve merging
data consider the following data sets, where 'id1' and 'id2' include the ids of data.

```
raw_data = {'id1': range(4),'income': [10,12,14,16]}
dat1 =pd.DataFrame(raw_data, columns = ['id1', 'income'])

raw_data = {'id2': range(6),'pay': [9,11,13,15,17,19]}
dat2 =pd.DataFrame(raw_data, columns = ['id2', 'pay'])
```

Obviously the id variable are not the same, they can be
compared using

```
dat1['id1'].isin(dat2['id2']).value_counts()
dat2['id2'].isin(dat1['id1']).value_counts()
```

`pd.merge` can merge different data-frames, the merging can be done based on the identities of left dataset, if there is no match in the right file, Python adds `NaN`.

```
result = pd.merge(dat1, dat2, left_on='id1', right_on='id2',how='left')
```
On contrary, one can the right dataset as matching,

```
result = pd.merge(dat1, dat2, left_on='id1', right_on='id2',how='right')
```

Since the ids are not the same, one can do merging based on the
intersection of the ids,

```
result = pd.merge(dat1, dat2, left_on='id1', right_on='id2',how='inner')
```

Merging can also be done based on the
union of the ids,

```
result = pd.merge(dat1, dat2, left_on='id1', right_on='id2',how='outer')
```

Note: If the names of id variables are the same in the both datasets, you can use ```on=id_name``` instead  ```left_on=``` and ```right_on=```.

Note: if you want to identify where the date in rows are from, add  argument ```indicator=True```, then new column named `_merge` would be added to the merged data which show its originate.

```
result = pd.merge(dat1, dat2, left_on='id1', right_on='id2',how='outer', indicator=True)
```

To combine row-wise, use `concat`.
```
result = pd.concat([dat1, dat2],axis=1)
```

## Crosstab
Consider housing_median_age and otal_rooms, group them using their quantile, then find the cross tabulate of them,

```
CHT['houlev'] = ''
C1=CHT.housing_median_age<=CHT.median_income.quantile(.3)
C2=CHT.housing_median_age>=CHT.median_income.quantile(.7)
CHT.loc[C1,'houlev']='L'
CHT.loc[~C1&~C2,'houlev']='M'
CHT.loc[C2,'houlev']='H'

CHT['roomlev'] = ''
C1=CHT.total_rooms<=CHT.total_rooms.quantile(.3)
C2=CHT.total_rooms>=CHT.total_rooms.quantile(.7)
CHT.loc[C1,'roomlev']='L'
CHT.loc[~C1&~C2,'roomlev']='M'
CHT.loc[C2,'roomlev']='H'

pd.crosstab(CHT.roomlev, CHT.houlev, margins=True)
```

Now use `famlev` as third variable and find their cross tab.

```
pd.crosstab([CHT.roomlev, CHT.houlev], CHT.famlev, margins=True)
```

## Missing Value
Missing value can be handled using ```.fillna(new_value))```. The following scripts generate a data set with missing values, then they fill with the mean of other values.

```
raw_data = {'income': [10,np.nan,14,16],'pay': [9,11,13,np.nan]}
dat =pd.DataFrame(raw_data, columns = ['income','pay'])

dat.income.fillna(dat.income.mean())
dat.fillna(dat.mean())
```

```
df.notnull()
df.isnull()
```

[pandas](https://pandas.pydata.org/pandas-docs/stable/missing_data.html) defines different procedures for filling missing, the following code interpolate the NaN.

```
dat.interpolate()
```

## Pipeline 
Pipeline in pandas allows to build a sequence of function to run in order on data-frame. 

```
def categ(x,col):
  x[col].quantile(.3)
  x['lev'] = ''
  C1=x[col]<=x[col].quantile(.3)
  C2=x[col]>=x[col].quantile(.7)
  x.loc[C1,'famlev']='L'
  x.loc[~C1&~C2,'famlev']='M'
  x.loc[C2,'famlev']='H'
  return x

def cv(x):
 return (np.mean(x)/np.var(x))

(
    CHT.pipe(categ, col='median_income') pipe(cv)
)
```



## Save
One of best format for saving data set is CSV data, the following script save the data as CSV without add row number and name, index=False.

```
CHT.to_csv("CHT.csv", index=False, encoding='utf8')
CHT.to_excel("CHT.xlsx")
```

## Good References  
http://jonathansoma.com/lede/foundations/ <br>
https://github.com/TomAugspurger/effective-pandas <br>
https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/ <br>
https://chrisalbon.com/python/data_wrangling/pandas_crosstabs/

